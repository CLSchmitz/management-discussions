# management-discussions
Can a Doc2Vec Vectorization of Management Discussions from 10-K's predict anything about the firm?

This is a refined version of a project initially submitted for a course in financial management as part of our masters in data science. The aim was to download 10-K reports, along with their management discussions, and see whether a model predicting a given indicator from a 10-K was improved by a doc2vec vectorization of the management discussion. The (loose) theory behind it was that "financials don't tell the whole story", and that perhaps soft things like tone and language used in the discussion could give some insight beyond what the numbers show.

A few things held the project back. The first big bottleneck was the data gathering, in which I wasn't really involved, but which took a lot of fidgeting with APIs and regex. The dataset is in the repo and probably the project's biggest asset to anyone looking to do some NLP. One thing we tried to do was calculate year-on-year deltas in the financial indicators where two consecutive reports were available (we usually failed to get those). We initially tried to use one of these deltas as the y-variable, but it thinned the dataset too much because of the NA's, and there was also no notable correlation between the remaining deltas and any other variable. We ended up just using the market cap of the firm as the y-variable, as this was the only one where results were decent with quick and dirty models, and we'd resigned to making this a PoC at this point.

The packages used in the project: numpy and pandas, of course; gensim for the text cleaning and doc2vec step; sklearn for the model; SHAP for some model explanations. We converted the cleaned management discussions into 30-dimension doc2vec vectors, and trained 3 models: (1) just the quant indicators, (2) just the doc2vec vectors, (3) both sets combined. The model building and optimization was absolutely not the focus of the project, and is the key area in which it could be improved without much effort. We used a random forest, for explainability reasons, with fairly standard hyperparameters.

So did it work? Keeping in mind how much was tweaked to produce presentable results, it did! Test R2 for the Quant, NLP, and combined models was 0.83, 0.81, and 0.87, respectively. With no further insights, these look impressive, even considering what a mess of cherry-picking the target variable and data cleaning underlies them. There's certainly significant room upwards for an optimized model here, too. However, the feature importance analysis I conducted in the jupyter notebook tells a different story, suggesting that only a handful of vectors were important in the NLP model, and even those diminished in importance in the full model.

Nonetheless: the document vectors improved the overall model, and more importantly, for some reason, the test R2 on only the document vectors was 0.81. This is despite taking out things like numbers and tables from the management discussions. I do not see an explanation for this beyond doc2vec encoding some useful information.
